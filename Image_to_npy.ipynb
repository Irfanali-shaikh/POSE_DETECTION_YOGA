{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47eb2773-f572-4095-bbc0-8bb537889938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\pose_detection\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce03d2b-87b6-42f5-8a9e-bc462e41b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,GRU,Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5b0412-4702-48cc-838e-e2da583911ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b2eb64-5459-4110-9e7c-dfceb3f903d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_image = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.7,min_tracking_confidence=0.7)\n",
    "\n",
    "# Initialize mediapipe drawing class - to draw the landmarks points.\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f831881-01cb-45ba-9567-d5717900f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPose(image_pose, pose, draw=False, display=False):\n",
    "    \n",
    "    original_image = image_pose.copy()\n",
    "    image_in_RGB = cv2.cvtColor(image_pose, cv2.COLOR_BGR2RGB)\n",
    "    resultant = pose.process(image_in_RGB)\n",
    "    if resultant.pose_landmarks and draw:    \n",
    "        mp_drawing.draw_landmarks(image=original_image, landmark_list=resultant.pose_landmarks,\n",
    "                                  connections=mp_pose.POSE_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,255,255),\n",
    "                                                                               thickness=3, circle_radius=3),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(49,125,237),\n",
    "                                                                               thickness=2, circle_radius=2))\n",
    "\n",
    "    if display:\n",
    "            \n",
    "            plt.figure(figsize=[22,22])\n",
    "            plt.subplot(121);plt.imshow(image_pose[:,:,::-1]);plt.title(\"Input Image\");plt.axis('off');\n",
    "            plt.subplot(122);plt.imshow(original_image[:,:,::-1]);plt.title(\"Pose detected Image\");plt.axis('off');\n",
    "    \n",
    "    else:\n",
    "        pose_ = np.array(([[res.x,res.y,res.z,res.visibility] for res in resultant.pose_landmarks.landmark] )).flatten() if resultant.pose_landmarks else np.zeros(1404) \n",
    "        return np.concatenate([pose_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d726ded4-68b6-4771-a7a7-3c99b4e68516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\pose_detection\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('Pose_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57afb8-26d3-468a-a84f-f20aa83a9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r'E:\\Pose Detection\\dummy\\farid.jpg'\n",
    "output = cv2.imread(image_path)\n",
    "cc = detectPose(output, pose_image, draw=True, display=False)\n",
    "detectPose(output, pose_image, draw=True, display=True)\n",
    "inverted_labels = {'downdog': 0, 'goddess': 1, 'plank': 2, 'tree': 3, 'warrior2': 4} {'downdog': 0, 'goddess': 1, 'plank': 2, 'tree': 3, 'warrior2': 4}\n",
    "labels = {}\n",
    "for i in inverted_labels.items():\n",
    "    labels[i[1]] = i[0]\n",
    "labels\n",
    "\n",
    "labels[np.argmax(mod.predict(cc.reshape(-1,132)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca387164-e6ee-408a-8c2b-fe131cbc57ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'downdog', 1: 'goddess', 2: 'plank', 3: 'tree', 4: 'warrior2'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_labels = {'downdog': 0, 'goddess': 1, 'plank': 2, 'tree': 3, 'warrior2': 4}\n",
    "labels = {}\n",
    "for i in inverted_labels.items():\n",
    "    labels[i[1]] = i[0]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da70b079-4e95-4bbf-87f9-7b304cde828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_frames(paths): \n",
    "    pose = [0]\n",
    "    frames = 0\n",
    "    cap = cv2.VideoCapture(paths)\n",
    "    while True: \n",
    "        _,frame = cap.read()\n",
    "        image = detectPose(frame, pose_image, draw=True, display=False)\n",
    "        lab = labels[np.argmax(model.predict(image.reshape(-1,132)))]\n",
    "        print(lab)\n",
    "        '''\n",
    "        word = list(set(pose.append(lab)))\n",
    "        cv2.rectangle(image,(0,0),(640,40),(0,0,0),-1)\n",
    "        cv2.putText(image,''.join(word),(3,30),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),1,cv2.LINE_AA)\n",
    "        ret, buffer = cv2.imencode('.jpg', image)\n",
    "        '''\n",
    "        if _:\n",
    "            cv2.imshow('Displaying image frames from a webcam', buffer)\n",
    "        if cv2.waitKey(25) == 27:\n",
    "            break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        #frame = buffer.tobytes()\n",
    "        #yield (b'--frame\\r\\n'b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c56fddf-aeab-415d-9090-1092ddc92d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "goddess\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'imshow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgen_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 17\u001b[0m, in \u001b[0;36mgen_frames\u001b[1;34m(paths)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mword = list(set(pose.append(lab)))\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mcv2.rectangle(image,(0,0),(640,40),(0,0,0),-1)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mcv2.putText(image,''.join(word),(3,30),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),1,cv2.LINE_AA)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03mret, buffer = cv2.imencode('.jpg', image)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _:\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mimshow\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisplaying image frames from a webcam\u001b[39m\u001b[38;5;124m'\u001b[39m, buffer)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m waitKey(\u001b[38;5;241m25\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imshow' is not defined"
     ]
    }
   ],
   "source": [
    "gen_frames(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be3436-3bc7-4ea0-bd50-98e06e006967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
